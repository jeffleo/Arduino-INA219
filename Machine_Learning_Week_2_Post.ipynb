{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 3244 - Machine Learning - Week 2 Post",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "kzgobjWkOofk",
        "_96b5EKHAaGF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jeffleo/Arduino-INA219/blob/master/Machine_Learning_Week_2_Post.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "uvf81x6Vrysn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Machine Learning](https://www.comp.nus.edu.sg/~kanmy/courses/3244_1810/cs3244banner.png)\n",
        "---\n",
        "See [Credits](https://colab.research.google.com/drive/10T1tSBCwsc1lOp7XXV2ED3P0nAppr2Tq#scrollTo=-kGaO2wXmyVP) for acknowledgements and rights.\n",
        "For NUS class credit, you'll need to download this notebook, complete all of the exercises in it (search for \"Your Turn\"), and submit the notebook online in IVLE by  **$\\large\\color{red}{\\sf Sun, 26\\:Aug\\:2018, 23:59\\: SGT}$**.  You must work in pairs or triples for all the in-class notebook assignments in class, unless otherwise specified by the lecturer(s).  \n",
        "\n",
        "**You must acknowledge that your submitted notebook is your team's independent work, accomplished partially during our in-class session; see below at the end.**\n"
      ]
    },
    {
      "metadata": {
        "id": "C7uUlXwCuvLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Learning Outcomes for this Week**\n",
        "\n",
        "After finishing the in-class exercises and post-class videos, you should be able to:\n",
        "* Understand Bayes' Rule and its application to Naïve Bayes;\n",
        "* Understand distance metrics and its application to $k$ Nearest Neighbors;\n",
        "* Implement both Naïve Bayes and $k$ Nearest Neighbors learning algorithms in pseudocode;\n",
        "* Know the training and testing times for both of the above algorithms;\n",
        "* Understand and apply basic linear algebra operations and know its terminology (rank, determinant, trace; vector, matrix, tensor; \\*Hessian, \\*Jacobian).\n"
      ]
    },
    {
      "metadata": {
        "id": "U4Is6Hg3EDf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# In Class Session\n",
        "# 1 Naïve Bayes"
      ]
    },
    {
      "metadata": {
        "id": "C448KE0XrkqG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Today, we are going to work on implementing Naïve Bayes from scratch.  We'll be using the dataset that you are already familiar with from the pre-class video, the _Play Golf_ dataset."
      ]
    },
    {
      "metadata": {
        "id": "8aIsSN3uvKtP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Let's create the Play Golf dataset from scratch\n",
        "pg_data = np.array([['Rainy','Hot','High','False','No'],\n",
        "              ['Rainy','Hot','High','True','No'],\n",
        "              ['Overcast','Hot','High','False','Yes'],\n",
        "              ['Sunny','Mild','High','False','Yes'],\n",
        "              ['Sunny','Cool','Normal','False','Yes'],\n",
        "              ['Sunny','Cool','Normal','True','No'],\n",
        "              ['Overcast','Cool','Normal','True','Yes'],\n",
        "              ['Rainy','Mild','High','False','No'],\n",
        "              ['Rainy','Cool','Normal','False','Yes'],\n",
        "              ['Sunny','Mild','Normal','False','Yes'],\n",
        "              ['Rainy','Mild','Normal','True','Yes'],\n",
        "              ['Overcast','Mild','High','True','Yes'],\n",
        "              ['Overcast','Hot','Normal','False','Yes'],\n",
        "              ['Sunny','Mild','High','True','No']])\n",
        "pg = pd.DataFrame(pg_data)\n",
        "# add column headers to our data\n",
        "pg.columns = [\"outlook\", \"temperature\", \"humidity\", \"windy\",\"play_golf\"]\n",
        "# Partition the features from the class to predict\n",
        "pg_X = pg[pg.columns[pg.columns != \"play_golf\"]].copy()\n",
        "pg_y = pg[\"play_golf\"].copy()\n",
        "# check that that worked by printing the first few rows\n",
        "pg.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3KAYSXsbrlC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Recall that in Naïve Bayes, we use Bayes' Rule: $P(y|x)=\\frac{P(x|y)\\times P(y)}{P(x)}$, and that we make a naïve assumption that all of the features of $x$ are independent of each other.  We then change the right hand side (RHS) of the rule to $\\frac{\\sum_i^nP(x_i|y)\\times P(y)}{P(x)}$.  \n",
        "\n",
        "We need statistics of the prior probability $P(y)$ and all of the conditional probabilities for all $n$ features of $x$: $P(x_1|y)$ through $P(x_n|y)$.  We don't need the denominator $P(x)$, the probability of the data, because the probabilities will affect all outcomes equally.\n",
        "\n",
        "Let's go ahead and get the prior probabilities of the class labels $P(y)\\in\\{Yes,No\\}$."
      ]
    },
    {
      "metadata": {
        "id": "OcpAHcpl6pbE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's calculate the priors.  They should be probabilities between 0 and 1. \n",
        "# First calculate the number of rows in the data table.\n",
        "num_rows = len(pg)\n",
        "\n",
        "# We'll use a hashtable to store the values of the priors.  Not efficient but simple.\n",
        "\n",
        "# the dataframe makes it easy to check whether rows or cells meet a condition.\n",
        "priors = { 'Yes': np.sum(pg['play_golf']=='Yes'),\n",
        "           'No':  np.sum(pg['play_golf']=='No')}\n",
        "\n",
        "# Done! Let's check...\n",
        "print (\"Prior probability of 'Play Golf == Yes' = %f\" % priors['Yes'])\n",
        "\n",
        "# Ooo, my bad.  These are counts, not probabilities yet.  \n",
        "# Your Turn: Can you fix lines 8 and 9?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hFj4hwceCYmq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we are going to calculate our conditional probabilities on the individual features of $x$, i.e., $P(x_i|y)$.  \n",
        "We'll start by counting how many times for the individual features of $x$ occur conditioned on the specific target $y$ value. "
      ]
    },
    {
      "metadata": {
        "id": "JAVncJ7ACWQa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's calculate the conditional probabilities.  They should also be probabilities between 0 and 1.  \n",
        "# We'll use a hashtable of two hashtables to store the values.\n",
        "# The first level we'll store the hashes for the output conditions, \n",
        "# and in the second level we'll store counts for the input feature values such as such as 'outlook==Rainy'\n",
        "likelihoods = {'Yes':{}, 'No':{}}\n",
        "\n",
        "likelihoods['Yes']['outlook==Rainy']    = len(pg[(pg['outlook']=='Rainy') & (pg['play_golf']=='Yes')])    \n",
        "likelihoods['Yes']['outlook==Sunny']    = len(pg[(pg['outlook']=='Sunny') & (pg['play_golf']=='Yes')])    \n",
        "likelihoods['Yes']['outlook==Overcast'] = len(pg[(pg['outlook']=='Overcast') & (pg['play_golf']=='Yes')]) \n",
        "likelihoods['Yes']['temperature==Hot']  = len(pg[(pg['temperature']=='Hot') & (pg['play_golf']=='Yes')])  \n",
        "likelihoods['Yes']['temperature==Mild'] = len(pg[(pg['temperature']=='Mild') & (pg['play_golf']=='Yes')]) \n",
        "likelihoods['Yes']['temperature==Cool'] = len(pg[(pg['temperature']=='Cool') & (pg['play_golf']=='Yes')]) \n",
        "likelihoods['Yes']['humidity==High']    = len(pg[(pg['humidity']=='High') & (pg['play_golf']=='Yes')])    \n",
        "likelihoods['Yes']['humidity==Normal']  = len(pg[(pg['humidity']=='Normal') & (pg['play_golf']=='Yes')])  \n",
        "likelihoods['Yes']['windy==True']       = len(pg[(pg['windy']=='True') & (pg['play_golf']=='Yes')])       \n",
        "likelihoods['Yes']['windy==False']      = len(pg[(pg['windy']=='False') & (pg['play_golf']=='Yes')])      \n",
        "\n",
        "likelihoods['No']['outlook==Rainy']    = len(pg[(pg['outlook']=='Rainy') & (pg['play_golf']=='No')])    \n",
        "likelihoods['No']['outlook==Sunny']    = len(pg[(pg['outlook']=='Sunny') & (pg['play_golf']=='No')])    \n",
        "likelihoods['No']['outlook==Overcast'] = len(pg[(pg['outlook']=='Overcast') & (pg['play_golf']=='No')]) \n",
        "likelihoods['No']['temperature==Hot']  = len(pg[(pg['temperature']=='Hot') & (pg['play_golf']=='No')])  \n",
        "likelihoods['No']['temperature==Mild'] = len(pg[(pg['temperature']=='Mild') & (pg['play_golf']=='No')]) \n",
        "likelihoods['No']['temperature==Cool'] = len(pg[(pg['temperature']=='Cool') & (pg['play_golf']=='No')]) \n",
        "likelihoods['No']['humidity==High']    = len(pg[(pg['humidity']=='High') & (pg['play_golf']=='No')])    \n",
        "likelihoods['No']['humidity==Normal']  = len(pg[(pg['humidity']=='Normal') & (pg['play_golf']=='No')])  \n",
        "likelihoods['No']['windy==True']       = len(pg[(pg['windy']=='True') & (pg['play_golf']=='No')])       \n",
        "likelihoods['No']['windy==False']      = len(pg[(pg['windy']=='False') & (pg['play_golf']=='No')])      \n",
        "\n",
        "# Check that our calculation is right\n",
        "print(\"Likelihood of it being sunny, given we are playing golf = %f\" % likelihoods['Yes']['outlook==Sunny'])\n",
        "\n",
        "# That's not right.  Ah... We forgot to normalise by the probability of the target class.  \n",
        "# Your Turn: Can you fix the above sets of lines 7-16 and 18-27 by dividing through by the appropriate amount?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydn_bCkmF4Xm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! We have everything we need.  It's sunny today outside.  What's the posterior probability that we'll go out and play golf? i.e., $P(Yes|outlook=Sunny)$.  Note the difference between the previous likelihood and what we want to calculate now."
      ]
    },
    {
      "metadata": {
        "id": "SoBD8p6oI3qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Posterior* probability we will play golf when it's sunny = %f\" % (likelihoods['Yes']['outlook==Sunny'] * priors['Yes']))\n",
        "print(\"Posterior* probability we won't play golf when it's sunny = %f\" % (likelihoods['No']['outlook==Sunny'] * priors['No']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhpWc-rfMeOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above looks strange doesn't it?  The probabilities don't sum to unity (1).  If they were really posterior probabilities, they would have to add up.  We put \"Posterior$\\color{red}{*}$\" instead of just \"Posterior\" to alert you to this.  This is because we didn't actually divide through by the data probabilities, the marginals, as in the slides, as they will affect both outcomes equally.  \n",
        "\n",
        "So it looks like we'll play golf after all.  \n",
        "\n",
        "Wait... Now that we've gone outside, I notice that the air is still and that it's humid and the temperature is hot.  That's Singapore for you.  What's my \"posterior\" probability now? "
      ]
    },
    {
      "metadata": {
        "id": "xTONRzKQF1qV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Your Turn: fill in the calculations\n",
        "posterior_yes_SHHF = 1 # Fix this line\n",
        "posterior_no_SHHF = 1  # Fix this one too\n",
        "\n",
        "print(\"Posterior* probability we will play golf when it's sunny, not windy, humid and hot = %f\" % posterior_yes_SHHF)\n",
        "print(\"Posterior* probability we won't play golf when it's sunny, not windy, humid and hot = %f\" % posterior_no_SHHF)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CA-AJscQPFSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "_Aiya!_  Guess I'll stay inside and do some machine learning instead. ^\\_^ \n",
        "\n",
        "Maybe we can fix the above to calculate actual probabilities for these values.  We'll just have to normalise them against each other.  That's the last part for today."
      ]
    },
    {
      "metadata": {
        "id": "BbOrSm2MQHFI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Your turn: Calculate the actual probability for our case of playing golf when it's sunny, still, humid and hot.\n",
        "# Store it in the variable \"Q05\"\n",
        "\n",
        "Q05 = 1 # Fix this line\n",
        "\n",
        "print(\"Actual posterior probability we will play golf when it's sunny, not windy, humid and hot = %f\" % Q05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dg1ln85qqEIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Optional)  We said that Naïve Bayes can only handle enumerated features and labels: see the summary slide in the Week 2 Pre videos.  Yet, the sklearn Naïve Bayes that we used in the pre class exercise handled all of the continuous features.  There's some contradiction here.  Let's pause for a moment and think what could be the difference.  Feel free to look it up on the Web (but then please follow best practice and credit where you found the information in the _References_ part of your Declaration of Individual Work )"
      ]
    },
    {
      "metadata": {
        "id": "a4iNLhyoTdYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your Turn (Optional): Why is there a discrepancy between the lecture and the pre-class exercise?  What does **sklearn**'s Naïve Bayes do that we didn't?  "
      ]
    },
    {
      "metadata": {
        "id": "5GWh9D9QFcGF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Post-Class Work\n",
        "\n",
        "\n",
        "You will have to watch the post-class videos on the lecture topics introduced today, then attempt the following exercises.  "
      ]
    },
    {
      "metadata": {
        "id": "hZX4JTdyIRuQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Prerequisites\n",
        "\n",
        "This week is also a bit special.  This is because this week we want you to devote some time to reviewing the prerequisites for this course.  Everyone has slightly different foundations coming into this course, so it's important to attempt to put us all in the same square with respect to our own prior knowledge. \n",
        "\n",
        "**Linear Algebra Review**.\n",
        "* For those who have a good grasp of Linear Algebra already, you may just want to refresh yourself using [Chapter 2 Linear Algebra](http://www.deeplearningbook.org/contents/linear_algebra.html) of the Goodfellow et al. _[Deep Learning](http://www.deeplearningbook.org/)_ book (The book itself is very good, we'd recommend it for any of you wanting to learn more about deep learning, plus it's freely accessible online).\n",
        "* If you're a little shakier with your LA foundations, you can work your way back up.  You should be familar with Eigendecomposition from your previous linear algebra course.  Have a look at 3Blue1Brown's [Essence of Linear Algebra](http://3b1b.co/eola) playlist.  This is a beautiful series of videos that have been [animated in Python](https://github.com/3b1b/manim) (go Python!) by its creator.  Watch what you need to understand the penultimate [Eigenvectors and eigenvalues](https://www.youtube.com/watch?v=PFDu9oVAE-g&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=14).  As stated by the 3Blue1Brown, Eigen-\"things\" are not too difficult to understand once you have a solid foundation on linear transformations, determinants, linear systems and change of basis.\n",
        "* Review the textbook from _[Immersive Math](http://immersivemath.com/ila/)_.  This is a recent, online textbook for linear algebra that is close to finished.  Many of the linear algebra concepts will be much better explained by this textbook through its 3D interactive animations.\n",
        "\n",
        "**Probability and Statistics Review**\n",
        "These two terms are inverse processes between data and a model.  _Probability_ is the process of analyzing a model to discover what data it will generate; _Statistics_ uses data to generate a model.  The second definition is close to what we mean by ML, so it's no surprise that statistics is a primary contributor to the ML world; in fact, in much more rigorous detail than we will go over in this course.   \n",
        "* It's good to have a good grasp of the [normal distribution](http://www.statisticshowto.com/probability-and-statistics/normal-distributions/).  I'd suggest being familiar with its properties.  These will be handy when we talk about random noise affecting learned models.\n",
        "* For this lecture where we discuss Bayes' Rule in part, understanding different parts that make up the posterior, and how it can be calculated from the data likelihoods, marginals and priors."
      ]
    },
    {
      "metadata": {
        "id": "u94aFdHLOuB5",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Your Turn: I actually reviewed the resources above to my satisfaction.\n",
        "Q06 = 'Yes I did' #@param [\"Yes I did\", \"No I didn't\", \"I wanted to ...\", \"I did a cursory review\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzgobjWkOofk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Quiz Time (hidden until you open it) \n",
        "Now that you think you've done an adequate review; let's have a mock test (You can be honest when the stakes are zero, these questions have zero weight).  When you're ready, open up the below section and take the mini-quiz.  Resist the temptation to look up answers on the Web.  But don't forget to take this mini-quiz before turning in your post-class notebook, ok?"
      ]
    },
    {
      "metadata": {
        "id": "V7I5MyIrnuA4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Your Turn: Is matrix multiplication associative?  Is it commutative?   \n",
        "Q07 = \"Yes to both\" #@param [\"Yes to both\", \"No to both\", \"Yes to associative and no to commutative\", \"Yes to commutative and no to associative\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-zuMz1QP_mo",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Your Turn: Explain what the L2 norm is in layman terms.   \n",
        "Q08 = 'Replace with your answer' #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xo3e1s3Efa-4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The _Monty Hall Problem_**.  \n",
        "\n",
        "![3 doors](https://upload.wikimedia.org/wikipedia/commons/3/3f/Monty_open_door.svg =600x300)\n",
        "\n",
        "(photo credits: in the public domain, authored by user Cepheus @ _Wikipedia_)\n",
        "\n",
        "Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\"  "
      ]
    },
    {
      "metadata": {
        "id": "dSU_mtY1eMhW",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Your Turn: Is it to your advantage to switch your choice? \n",
        "Q09 = \"Yes\" #@param [\"Yes\",\"No\",\"It depends\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nHGDKzwnpxSr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your Turn (Optional): From above Q09, why or why not? Justify, as you would to a peer in the class.  You may find it useful to use LaTeX math notation, which is easier learned from examples (you can see the source for the equations we used in our notebooks by double clicking cells in your copy of the notebook).\n",
        "\n",
        "(Optional) Replace with your answer..."
      ]
    },
    {
      "metadata": {
        "id": "y0Mkt1urPkPq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some of you may already be familiar with this problem from previous classes before university.  [Monty Hall](https://en.wikipedia.org/wiki/Monty_Hall) was a famous TV personality, hosting a game show _Let's make a deal_ in the 1960s to the 1980s.  He offered versions of this problem to contestants.  This problem flummoxed many members of the public, including mathematics Ph.D. holders and (famously, [Paul Ërdos](https://en.wikipedia.org/wiki/Paul_Erd%C5%91s)), who wrote in to [Marilyn von Savant](https://en.wikipedia.org/wiki/Marilyn_vos_Savant), the columnist who offered the correct solution to the problem, to tell her she was wrong.\n"
      ]
    },
    {
      "metadata": {
        "id": "JFCCE8XuYeeS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Post quiz, if you got stuck on any of these problems, you can look it up on the Web (don't forget to give credit in your _Declaration_), and discuss it on the discussion forum so that everyone can benefit."
      ]
    },
    {
      "metadata": {
        "id": "qonvX7YFt-fx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 $k$ Nearest Neighbors\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-ZBLpcOyuI5Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we discussed in the video $k$ NN doesn't have to do anything to set up the model $h_{\\theta}$ for training.  The parameters ${\\theta}$  in $k$ NN is actual the _hyperparameter_ choices made for the distance metric and the number of clusters $k$.  \n",
        "\n",
        "Let's examine the Iris dataset which has samples of iris flowers attributed to 3 different species of iris.  For simplifiy, we'll study only the first two features, and examine how the decision boundary changes with different values of $k$.\n",
        "\n",
        "Feel free to look at other features and change the hyperparameter values - the code is provided to you for studying and experimentation. "
      ]
    },
    {
      "metadata": {
        "id": "XzRn9jvquLnc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This code is a modification of the sample code for kNN in sklearn\n",
        "# Modified by Min, Aug 2018.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import neighbors, datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# we only take the first two features. \n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Create color maps\n",
        "cmap_light = ListedColormap(['#FFCCCC', '#CCFFCC', '#CCCCFF'])\n",
        "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
        "\n",
        "step_size = .02  # step size in the mesh\n",
        "\n",
        "plt.figure(figsize=(10,10)) # make a bigger plot for our subplots\n",
        "for i in range(1,10):\n",
        "  ## map a subplot i to a certain selection of k\n",
        "  # Your turn (optional): modify and experiment\n",
        "  n_neighbors = 4*i - 3 \n",
        "  plt.subplot(3, 3, i) \n",
        "  # we create an instance of Neighbours Classifier and fit the data.\n",
        "  clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
        "  clf.fit(X, y)\n",
        "\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, step_size),\n",
        "                       np.arange(y_min, y_max, step_size))\n",
        "  Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
        "\n",
        "  # Plot also the training points\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
        "              edgecolor='k', s=20)\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "  plt.title(\"%i-NN\" % n_neighbors)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9rCzXERkqz74",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 Scaling and Normalizing Data\n"
      ]
    },
    {
      "metadata": {
        "id": "mwMXwfm_YGE6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Certain machine learning algorithms are sensitive to distance changes, as we have discussed in the post-class video lecture.  Let's see the effect that normalization and scaling has on the wine quality dataset.  Let's first get it set up."
      ]
    },
    {
      "metadata": {
        "id": "mOVXVUi6fbCA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "# Let's read the data in as a data frame, equivalent to our (X,Y) data matrix. \n",
        "# We'll use another variable even though it's the same data as in our W2 Pre notebook.\n",
        "wq = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',sep=';') # Separate on semicolons\n",
        "\n",
        "wq['good wine'] = np.where(wq['quality']>=7, \"Good\", \"Not Good\")\n",
        "wq.drop('quality', axis=1, inplace=True) \n",
        "wq_X = wq[wq.columns[wq.columns != 'good wine']].copy() \n",
        "wq_y = wq['good wine'].copy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nD9Q--26gYv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Scaling usually is done using min–max scaling, where we take the minimum and maximum values and assign them to 0 and 1, respectively.  Every other value gets mapped in between by a linear transformation.\n",
        "\n",
        "Often, instead of scaling, we will do normalization, which takes a feature (column) $x_i$ and rescales it so that it's values will have the properties of a standard normal distribution with $\\mu=0$ and $\\sigma = 1$.  Both of these operations are available in sklearn.  Let's see the effect of normalization has on the wine quality dataset:"
      ]
    },
    {
      "metadata": {
        "id": "bK4Gx3lemIDF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train) # rescale for \\mu = 0 and \\sigma = 1, and remember the parameters for the scaling\n",
        "X_test_scaled = scaler.transform(X_test) # Apply the transform with the **same parameters from training** to the test data.  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QY-YZ35bhpa7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fa_mean =        X_train['fixed acidity'].mean()\n",
        "fa_mean_scaled = X_train_scaled[:,0].mean() # fixed acidity is column 0, when we transformed the data, the output doesn't have column information any more\n",
        "fa_std =        X_train['fixed acidity'].std()\n",
        "fa_std_scaled = X_train_scaled[:,0].std() \n",
        "\n",
        "va_mean =        X_train['volatile acidity'].mean()\n",
        "va_mean_scaled = X_train_scaled[:,1].mean() # volatile acidity is column 1, when we transformed the data, the output doesn't have column information any more\n",
        "va_std =        X_train['volatile acidity'].std()\n",
        "va_std_scaled = X_train_scaled[:,1].std() \n",
        "\n",
        "# check\n",
        "print (\"Fixed Acidity mean and standard deviation: %f (%f)\" % (fa_mean,fa_std))\n",
        "print (\"Fixed Acidity (normalized) mean and standard deviation: %f (%f)\" % (fa_mean_scaled,fa_std_scaled))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZODfhtYJeEu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "%matplotlib inline\n",
        "\n",
        "# plot both the normalized and unnormalized data in the same plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_train['fixed acidity'], X_train['residual sugar'],\n",
        "            color='green', label='Unnormalized', alpha=0.5)\n",
        "plt.scatter(X_train_scaled[:,0], X_train_scaled[:,1], color='red',\n",
        "            label='Normalized [$\\mu = 0, \\sigma = 1$]', alpha=0.3)\n",
        "plt.title('Fixed versus Volatile Acidity on the Wine Quality Dataset')\n",
        "plt.xlabel('Fixed Acidity')\n",
        "plt.ylabel('Volatile Acidity')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uLxdcbl6mH5B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get the machine learning algorithm\n",
        "from sklearn import neighbors\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
        "knn_model_scaled = knn.fit(X_train_scaled, y_train)\n",
        "print('1-NN accuracy (Scaled) for test set: %f' % knn_model_scaled.score(X_test_scaled, y_test))\n",
        "knn_model = knn.fit(X_train, y_train)\n",
        "print('1-NN accuracy (Non-scaled) for test set: %f' % knn_model.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOSsw8qSKdLY",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Your Turn: Why do you think scaling helps in this case to improve nearest neighbors?\n",
        "Q10 = 'Replace with your answer' #@param {type:\"string\"} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_SC6zmZUpd9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your Turn (Optional): From above Q10, why or why not? Justify, as you would to a peer in the class.  Do you think it always helps?  When would you use it?\n",
        "\n",
        "Replace with your answer..."
      ]
    },
    {
      "metadata": {
        "id": "cUInnvDVud53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 Curse of Dimensionality"
      ]
    },
    {
      "metadata": {
        "id": "unEAnIyZug1G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(Optional) Finally we include code for you to run and modify to check the _Curse of Dimensionality_.  Recall that the Curse of Dimensionality suggests that the larger the dimensional space you have in your dataset, the more difficult certain aspects of learning can become."
      ]
    },
    {
      "metadata": {
        "id": "E9xSXl9YujTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Try some on your own\n",
        "for dim in [1,2,3,4,8,10,100,200]:\n",
        "    ## choices of |X|\n",
        "    samples = 200\n",
        "    #samples = pow(2,dim)\n",
        "    X = np.random.uniform(size=(samples,dim))\n",
        "\n",
        "    ## choices of x\n",
        "    x = np.random.uniform(size=(dim))\n",
        "    # x = np.zeros(dim)\n",
        "    print(\"%d dimensions:\" % dim)\n",
        "    print(\"#   Generated %d points\" % samples)\n",
        "    # print(\"#   Test point: \", x)\n",
        "    dist_array = []\n",
        "    for j in range(len(X)):\n",
        "        dist = 0.0\n",
        "        for i in range(dim):\n",
        "    #        print (\"%f => %f \"% (x[i]-X[j][i],pow(x[i]-X[j][i],2)))\n",
        "            dist += pow(x[i]-X[j][i],2)\n",
        "    #    print (\"Sum distance %f => %f\" % (dist,pow(dist,0.5)))\n",
        "        dist = pow(dist,0.5)\n",
        "        dist_array.append(dist)\n",
        "    #    print (\"%d th point %s has distance %f\" % (j,X[j],dist))\n",
        "    distances = np.array(dist_array)\n",
        "    print (\"#   Average distance (STD) of test point to %d samples: %f (%f)\" % (samples,distances.mean(),distances.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXFs7a9O5dAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "This comes to the end of this class' notebook.   **Each member of your team must submit the notebook to IVLE workbin by the appropriate deadline of **$\\large\\color{red}{\\sf Sun, 26\\:Aug\\:2018, 23:59\\: SGT}$**.  Working on your own defeats the purpose of our flipped classroom exercises and is not allowed; you will be asked to work in a team if you come late.  This is necessary to let our automated grading programs mark your assignment.\n",
        "Don't forget to do your pre-class video watching and pre-class work in the subsequent week's notebook. "
      ]
    },
    {
      "metadata": {
        "id": "ct2qrSq3G9Fr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Declaration of Independent Work\n",
        "By entering my ID below, I certify that I completed my notebook independently of all others (except where sanctioned during in-class sessions), obeying the class policy outlined in the introductory lecture.  In particular, I am allowed to discuss the problems and solutions in this notebook, but have waited at least 30 minutes by doing other activities unrelated to class before attempting to complete or modify my answers as per the Pokémon Go rule.  You must sign **exactly one of the two below statements** with your Student ID.  Leave the other as-is.\n",
        "\n",
        "If you need to sign the second declaration of not following policy, please also complete the other two text boxes marked with \"Your Turn\"."
      ]
    },
    {
      "metadata": {
        "id": "BLBA6pMaqHZe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your Turn (References)**: I give credit where credit is due.  I acknowledge that I used the following websites or contacts to complete this assignment:\n",
        "* _Sample_. [Website 1](http://example.com), for following mathematical proofs.\n",
        "* _Sample_. My friend, John Doe, on the programming syntax.\n"
      ]
    },
    {
      "metadata": {
        "id": "2zR6BhDVOLZk",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title I followed class policy\n",
        "\n",
        "alt_sign1 = \"Your Student ID\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IjhOxfm6QWMo",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title I didn't follow class policy\n",
        "alt_sign2 = \"Your Student ID\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNsoVI9uQoPt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your Turn**: I didn't follow class policy because of ..."
      ]
    },
    {
      "metadata": {
        "id": "oYrQnwo-QveW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Your Turn**: I suggest I be graded as ..."
      ]
    },
    {
      "metadata": {
        "id": "-kGaO2wXmyVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Credits\n",
        "Authored by [Min-Yen Kan](http://www.comp.nus.edu.sg/~kanmy) and Chris Boesch (2018), affiliated with [WING](http://wing.comp.nus.edu.sg), [NUS School of Computing](http://www.comp.nus.edu.sg) and [ALSET](http://www.nus.edu.sg/alset).\n",
        "Licensed as: [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/ ) (CC BY 4.0).  Based partially on notebooks from [SarahG](https://www.kaggle.com/sgus1318/wine-quality-exploration-and-analysis/notebook) via Kaggle, [Nishank Kumar Sharma](https://medium.com/click-bait/wine-quality-prediction-using-machine-learning-59c88a826789) via Medium, Iris plot modified from tutorial on sklearn. \n",
        "Please retain and add to this credits cell if using this material as a whole or in part.   Credits for photos given in their captions."
      ]
    },
    {
      "metadata": {
        "id": "_96b5EKHAaGF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Grading Harness (not to be edited)"
      ]
    },
    {
      "metadata": {
        "id": "bSmE5K57AX3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Q00 = \"revision=1\"\n",
        "Q03 = priors['Yes']\n",
        "Q04 = likelihoods['Yes']['outlook==Sunny']\n",
        "\n",
        "# General\n",
        "if alt_sign1 == \"Your Student ID\" or alt_sign1==\"\":\n",
        "  if alt_sign2 == \"Your Student ID\" or alt_sign2==\"\": # both unsigned\n",
        "    matric = \"Unknown\"\n",
        "    status = \"unsigned\"\n",
        "  else: # Signed 2\n",
        "    matric = alt_sign2\n",
        "    status = \"did not follow\"\n",
        "else:\n",
        "  if alt_sign2 == \"Your Student ID\" or alt_sign2==\"\":\n",
        "    matric = alt_sign1\n",
        "    status = \"followed\" \n",
        "  else:\n",
        "    matric = alt_sign1 \n",
        "    status = \"signed both\"\n",
        "\n",
        "csvdata = [matric,status]\n",
        "csvdata.extend(value for name, value in sorted(locals().items(), key=lambda item: item[0]) if name.startswith('Q'))\n",
        "def replComma(Q): return Q.replace(',','') if isinstance(Q,str)  else Q \n",
        "csvdata = [replComma(Q) for Q in csvdata]\n",
        "print (csvdata)\n",
        "\n",
        "import json\n",
        "\n",
        "with open('results.json', 'w') as outfile:\n",
        "    json.dump(csvdata, outfile)\n",
        "    \n",
        "import re\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "\n",
        "def calculateNUSMatricNumber(id):\n",
        "  matches = re.match(r'^A\\d{7}.|U\\d{6,7}.',id)\n",
        "  if (matches):\n",
        "    originalMatch = id;\n",
        "    match = matches[0][:-1]\n",
        "    \n",
        "    #Discard 3rd digit from U-prefix NUSNET ID\n",
        "    if (match[0] == 'U' and len(match) == 8):\n",
        "      match = mathc[:3] + match[4]\n",
        "    weigths = { 'U':[0,1,3,1,2,7],'A':[1,1,1,1,1,1]}[match[0]]\n",
        "    sum = 0;\n",
        "    digits = match[-6:];\n",
        "    for i in range(6):\n",
        "      sum += weigths[i] * int(digits[i])     \n",
        "    return originalMatch == (match + 'YXWURNMLJHEAB'[sum %13])\n",
        "\n",
        "def checkNotebook():\n",
        "    if not 'csvdata' in globals():\n",
        "      print(\"Seems like you haven't run the Grading Harness. Just got to 'Runtime -> Run all' and run the entire notebook.\")\n",
        "      return display(HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Smiley_green_alien_sherlock.svg/200px-Smiley_green_alien_sherlock.svg.png\" alt=\"Sad\"><br> \\\n",
        "                          <i>Graphics in the public domain from the <a href=\"https://commons.wikimedia.org/wiki/User:LadyofHats\">LadyofHats</a> \\\n",
        "                          via <a href=\"https://commons.wikimedia.org/wiki/File:Smiley_green_alien_sherlock.svg\">Wikimedia Commons</a>.') )\n",
        "    elif csvdata[1] == \"signed both\":\n",
        "      print(\"Seems like you have signed both fields. That's a little confusing, please decide on one and leave the other empty.\")\n",
        "      return display(HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/Smiley_green_alien_dead_ninja.svg/200px-Smiley_green_alien_dead_ninja.svg.png\" alt=\"Confused\"><br> \\\n",
        "                           <i>Graphics in the public domain from the <a href=\"https://commons.wikimedia.org/wiki/User:LadyofHats\">LadyofHats</a> \\\n",
        "                           via <a href=\"https://commons.wikimedia.org/wiki/File:Smiley_green_alien_dead_ninja.svg\">Wikimedia Commons</a>.') )\n",
        "    elif(calculateNUSMatricNumber(csvdata[0])):\n",
        "      print(\"Great. Everything seems to be in place. You can download the notebook and submit it to IVLE.\")\n",
        "      return display(HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/79/Face-smile.svg/200px-Face-smile.svg.png\" alt=\"Good job\"><br>\\\n",
        "                          <i>Graphics in the public domain from the <a href=\"http://tango.freedesktop.org/The_People\">Tango Project</a> \\\n",
        "                          via <a href=\"https://commons.wikimedia.org/wiki/File:Face-smile.svg\">Wikimedia Commons</a>.') )\n",
        "    elif(csvdata[0] == 'Unknown'):\n",
        "      print(\"Seems like you forgot to enter your Student ID. Just enter it under 'Declaration of Independent Work' and try again (Runtime -> Run all).\")\n",
        "      return display(HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Smiley_green_alien_big_eyes.svg/200px-Smiley_green_alien_big_eyes.svg.png\" alt=\"Sad\"><br> \\\n",
        "                          <i>Graphics in the public domain from the <a href=\"https://commons.wikimedia.org/wiki/User:LadyofHats\">LadyofHats</a> \\\n",
        "                          via <a href=\"https://commons.wikimedia.org/wiki/File:Smiley_green_alien_big_eyes.svg\">Wikimedia Commons</a>.') )\n",
        "    else:\n",
        "      print(\"Whatever you entered under Declaration of Independent Work it's not a valid Student ID (A???????E). Please try again.\")\n",
        "      return display(HTML('<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/Smiley_green_alien_worried.svg/200px-Smiley_green_alien_worried.svg.png\" alt=\"Sad\"><br> \\\n",
        "                          <i>Graphics in the public domain from the <a href=\"https://commons.wikimedia.org/wiki/User:LadyofHats\">LadyofHats</a> \\\n",
        "                          via <a href=\"https://commons.wikimedia.org/wiki/File:Smiley_green_alien_worried.svg\">Wikimedia Commons</a>.') )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YrbbUhSUe3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Final Check"
      ]
    },
    {
      "metadata": {
        "id": "3dA5tpPNUbB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkNotebook()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}